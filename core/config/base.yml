base_model: NousResearch/Meta-Llama-3-8B-Instruct
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

# Quantization settings for 4-bit QLoRA
load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: bfloat16
strict: false

chat_template: llama3
datasets:
  dataset_prepared_path:
  val_set_size: 0.05
output_dir: miner_id_24

environment:
  trust_remote_code: true
  gpu_memory_limit: 80GiB
  sample_packing: true
  eval_sample_packing: true
  pad_to_sequence_len: true
  sequence_len: 512

adapter: qlora
lora_model_dir:
lora_r: 16
lora_alpha: 16
lora_dropout: 0.05
lora_target_linear: true

# Batch / accumulation (auto-find or manual)
auto_find_batch_size: true
gradient_accumulation_steps: 8
micro_batch_size: 2

# Training schedule
num_epochs: 3
learning_rate: 3e-4
weight_decay: 0.01
lr_scheduler: cosine_with_restarts
lr_scheduler_args:
  num_cycles: 1
warmup_ratio: 0.03

# Precision & performance
bf16: true
fp16: false
tf32: true

gradient_checkpointing: true
flash_attention: true
s2_attention: true

# Logging & evaluation
early_stopping_patience: 3
resume_from_checkpoint:
logging_steps: 50
save_steps: 1000
eval_steps: 1000
eval_max_new_tokens: 128

# Remove FSDP (simpler single-node training) and rely on QLoRA
fsdp: []
deepseed:
#  stage: 2 # enable if scaling larger than 8B
#  offload_optimizer: false
#  offload_parameters: false

# Liger plugin (optional)
plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_rope: true
liger_rms_norm: true
liger_glu_activation: true
liger_layer_norm: true
liger_fused_linear_cross_entropy: true

# WandB integration
wandb_project: Gradients-On-Demand
wandb_entity:
wandb_mode: online
wandb_run: your_name

# Hub settings (managed externally)
hub_model_id:
hub_strategy: checkpoint
hub_token:
