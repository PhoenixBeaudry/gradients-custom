base_model: NousResearch/Meta-Llama-3-8B-Instruct
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

# 4-bit QLoRA quantization
load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: bfloat16
strict: false

chat_template: llama3
datasets:
  dataset_prepared_path:
  val_set_size: 0.05
output_dir: miner_id_24

# Environment / tokenization
environment:
  trust_remote_code: true
  gpu_memory_limit: 80GiB
  sample_packing: true
  eval_sample_packing: true
  pad_to_sequence_len: true
sequence_len: 512

# QLoRA adapter settings
adapter: qlora
lora_model_dir:
lora_r: 16
lora_alpha: 16
lora_dropout: 0.05
lora_target_linear: true

# Batch sizing for lowest-loss focus
auto_find_batch_size: true

# Precision & compute optimizations
bf16: true
fp16: false
tf32: true
gradient_checkpointing: true
flash_attention: true

# Training schedule
epochs_or_steps_mode: epochs
num_epochs: 5
max_steps: 20000
learning_rate: 3e-4
weight_decay: 0.01
optimizer: adamw_8bit
max_grad_norm: 1.0

# LR schedule & warmup
lr_scheduler: cosine_with_restarts
lr_scheduler_args:
  num_cycles: 2
warmup_steps: 100

# Logging & evaluation
early_stopping_patience: 5
logging_steps: 500
save_steps: 2000
eval_steps: 2000
eval_max_new_tokens: 128

# Keep Liger plugin for throughput
plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_rope: true
liger_rms_norm: true
liger_glu_activation: true
liger_layer_norm: true
liger_fused_linear_cross_entropy: true

# Weights & hub (managed externally)
hub_strategy: checkpoint
hub_model_id:
hub_token:
